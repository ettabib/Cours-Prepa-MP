\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

{[}
{[}{]}
{[}

\subsubsection{13.1 Compléments sur la con\\\\jmathmathmathmathugaison}

\paragraph{13.1.1 Applications semi-linéaires}

Définition~13.1.1 Soit E et F deux \mathbb{C}-espaces vectoriels et u : E \rightarrow~ F. On
dit que u est semi-linéaire si elle vérifie

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) \forall~~x,y \in E, u(x + y) = u(x) + u(y)
\item
  (ii) \forall~\lambda~ \in \mathbb{C}, \\forall~~x \in
  E, u(\lambda~x) = \overline\lambda~u(x).
\end{itemize}

Remarque~13.1.1 Soit E un \mathbb{C}-espace vectoriel . On munit E d'une autre
structure d'espace vectoriel, notée \checkE en posant
\lambda~ ∗ x = \overline\lambda~x. Une application semi-linéaire de
E dans F n'est autre qu'une application linéaire de E dans
\checkF. Ceci permet d'appliquer aux applications
semi-linéaires la plupart des résultats sur les applications linéaires
en tenant compte des résultats suivants dont la démonstration est
élémentaire~:

\begin{itemize}
\item
  a) une famille (xi)i\inI d'éléments de E est libre
  (resp. génératrice, resp. base) dans \checkE si et
  seulement si~il en est de même dans E
\item
  b) \mathrmrg~
  \checkE(xi)i\inI
  = \mathrmrg~
  E(xi)i\inI, dim~
  \checkE = dim~ E
\item
  c) F est un sous-espace vectoriel de \checkE si et
  seulement si~c'est un sous-espace vectoriel de E
\item
  d) le théorème du rang s'applique aux applications semi-linéaires~; en
  particulier, si u : E \rightarrow~ F est semi-linéaire entre deux espaces de même
  dimension finie, alors u est in\\\\jmathmathmathmathective si et seulement si~elle est
  sur\\\\jmathmathmathmathective
\item
  e) si l'on définit A =\
  \mathrmMat (u,\mathcal{E},ℱ) par u(e\\\\jmathmathmathmath)
  = \\sum ~
  iai,\\\\jmathmathmathmathfi (notations évidentes) alors

  y = u(x) \Leftrightarrow Y =
  A\overlineX
\item
  f) la composée de deux applications semi-linéaires n'est pas
  semi-linéaire, mais au contraire linéaire.
\end{itemize}

\paragraph{13.1.2 Matrices con\\\\jmathmathmathmathuguées et transcon\\\\jmathmathmathmathuguées}

Définition~13.1.2 Soit A = (ai,\\\\jmathmathmathmath)1\leqi\leqm,1\leq\\\\jmathmathmathmath\leqn \in
M\mathbb{C}(m,n). On appelle matrice con\\\\jmathmathmathmathuguée de A la matrice
\overlineA =
(\overlineai,\\\\jmathmathmathmath)1\leqi\leqm,1\leq\\\\jmathmathmathmath\leqn \in
M\mathbb{C}(m,n).

Proposition~13.1.1 L'application
A\mapsto~\overlineA est un
automorphisme semi-linéaire de M\mathbb{C}(m,n). On a
\mathrmrg\overlineA~
= \mathrmrg~A. Si A \in
M\mathbb{C}(m,n) et B \in M\mathbb{C}(n,p), alors
\overlineAB =
\overlineA\,\overlineB.
Dans le cadre des matrices carrées, on a
\mathrm{det}~
\overlineA =
\overline\mathrm{det}~
A,
\mathrm{tr}\overlineA~
=
\overline\mathrm{tr}A~,
\chi\overlineA(X) =
\overline\chiA(X), A est inversible si et
seulement si~\overlineA est inversible, et dans ce
cas (\overlineA)^-1 =
\overlineA^-1.

Démonstration Vérification élémentaire laissée au lecteur.

Définition~13.1.3 Soit A = (ai,\\\\jmathmathmathmath)1\leqi\leqm,1\leq\\\\jmathmathmathmath\leqn \in
M\mathbb{C}(m,n). On appelle matrice transcon\\\\jmathmathmathmathuguée (ou matrice
ad\\\\jmathmathmathmathointe) de A la matrice A^∗ =
^t(\overlineA) =
\overline^tA =
(\overlinea\\\\jmathmathmathmath,i)1\leqi\leqm,1\leq\\\\jmathmathmathmath\leqn \in
M\mathbb{C}(n,m).

A partir des propriétés de A\mapsto~^tA
et de A\mapsto~\overlineA on
déduit facilement les propriétés suivantes

Proposition~13.1.2 L'application
A\mapsto~A^∗ est un isomorphisme
semi-linéaire involutif de M\mathbb{C}(m,n) sur M\mathbb{C}(n,m). On a
\mathrmrgA^∗~
= \mathrmrg~A. Si A \in
M\mathbb{C}(m,n) et B \in M\mathbb{C}(n,p), alors (AB)^∗ =
B^∗A^∗. Dans le cadre des matrices carrées, on a
\mathrm{det} A^∗~ =
\overline\mathrm{det}~
A,
\mathrm{tr}A^∗~ =
\overline\mathrm{tr}A~,
\chiA^∗(X) =
\overline\chiA(X), A est inversible si et
seulement si~A^∗ est inversible, et dans ce cas
(A^∗)^-1 = (A^-1)^∗.

Remarque~13.1.2 On prendra garde à la relation (\lambda~A)^∗ =
\overline\lambda~A^∗ en n'oubliant pas la
con\\\\jmathmathmathmathugaison.

\paragraph{13.1.3 Matrices hermitiennes, antihermitiennes}

Définition~13.1.4 Soit A \in M\mathbb{C}(n). on dit que A est hermitienne
(resp. antihermitienne) si A^∗ = A (resp. A^∗ =
-A).

Remarque~13.1.3 A = (ai,\\\\jmathmathmathmath) est hermitienne si et seulement
si~\forall~i,\\\\jmathmathmathmath, a\\\\jmathmathmathmath,i~ =
\overlineai,\\\\jmathmathmathmath. En particulier les
coefficients diagonaux ai,i doivent être réels

Théorème~13.1.3 Les ensembles des matrices hermitiennes et
antihermitiennes sont des \mathbb{R}~-sous-espaces vectoriels (mais pas des
\mathbb{C}-sous-espaces vectoriels) de M\mathbb{C}(n). On a

A\text hermitienne  \Leftrightarrow
iA\text antihermitienne

Si ℋn désigne le \mathbb{R}~-sous-espace vectoriel des matrices
hermitiennes, on a M\mathbb{C}(n) = ℋn \oplus~ iℋn.

Démonstration La vérification du premier point est élémentaire. Si on a
A = A1 + iA2 avec A1 et A2
hermitiennes, alors A^∗ = A1 - iA2 ce qui
donne A1 = 1 \over 2 (A + A^∗)
et A2 = 1 \over 2i (A - A^∗) et
démontre dé\\\\jmathmathmathmathà l'unicité de la décomposition. De plus la formule

A = 1 \over 2 (A + A^∗) + i 1
\over 2i (A - A^∗)

avec  1 \over 2 (A + A^∗) et  1
\over 2i (A - A^∗) qui sont hermitiennes
(facile) montre l'existence de la décomposition.

Remarque~13.1.4 On voit donc que contrairement aux matrices symétriques
ou antisymétriques qui sont de nature différentes, il n'y a pas de
différence essentielle entre matrices hermitiennes ou antihermitiennes~:
on passe des unes aux autres par multiplication par i, ce qui permet de
limiter l'étude aux matrices hermitiennes. Pour une telle matrice, les
formules \mathrm{det}~
A^∗ =
\overline\mathrm{det}~
A,
\mathrm{tr}A^∗~ =
\overline\mathrm{tr}A~,
\chiA^∗(X) =
\overline\chiA(X) montrent que
\mathrm{det}~ A \in \mathbb{R}~,
\mathrm{tr}~A \in \mathbb{R}~ et que
\chiA(X) \in \mathbb{R}~{[}X{]}.

{[}
{[}
