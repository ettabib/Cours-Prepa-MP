\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

{[}
{[}
{[}{]}
{[}

\subsubsection{3.2 Polynômes d'endomorphismes}

\paragraph{3.2.1 Généralités}

Soit E un K-espace vectoriel et u \in L(E). On pose u^0 =
\mathrmIdE et pour k ≥ 1, u^k = u
\cdot u^k-1. Si P \in K{[}X{]}, P =\
\sum ~
k=0^dakX^k, on pose

P(u) = \sum k=0^da~
ku^k

Proposition~3.2.1 L'application P\mapsto~P(u) est un
morphisme de K-algèbres de K{[}X{]} dans L(E). Son image K{[}u{]} est la
plus petite sous-algèbre de L(E) contenant u~; elle est commutative.

Démonstration Vérifications immédiates.

\paragraph{3.2.2 Idéal annulateur. Polynôme minimal}

Définition~3.2.1 On appelle idéal annulateur de u \in L(E) l'idéal
Iu = \P \in
K{[}X{]}∣P(u) = 0\. On dit
que u admet un polynôme minimal si
Iu\neq~\0\.
Dans ce cas Iu est engendré par un unique polynôme normalisé
\muu(X) appelé le polynôme minimal de u.

Exemple~3.2.1 Si E = K{[}X{]} et u :
P(X)\mapsto~XP(X), on a Q(u) :
P(X)\mapsto~Q(X)P(X) soit
Q(u)\neq~0 et donc u n'admet pas de polynôme
minimal. Ce phénomène ne peut pas se produire en dimension finie

Théorème~3.2.2 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors u admet un polynôme minimal.

Démonstration Premier argument~: comme dim~
K{[}X{]} = +\infty~ et dim~ L(E) \textless{} +\infty~,
l'application P(X)\mapsto~P(u) ne peut être
in\\\\jmathmathmathmathective. Deuxième argument~: comme dim~ L(E)
= n^2 (où n = dim~ E), la famille de
cardinal n^2 + 1,
(u^k)0\leqk\leqn^2 doit être liée~; il existe
donc
\lambda~0,\\ldots,\lambda~n^2~
non tous nuls tels que \lambda~0u^0 +
\\ldots~ +
\lambda~n^2u^n^2  = 0~; le polynôme
\lambda~01 +
\\ldots~ +
\lambda~n^2X^n^2  n'est pas nul et
il est dans Iu.

On suppose désormais dim~ E \textless{} +\infty~

Proposition~3.2.3 Soit F un sous-espace de E stable par u et v
l'endomorphisme de F induit par u. Alors \muv divise
\muu.

Démonstration On vérifie facilement que pour tout k dans \mathbb{N}~,
v^k est l'endomorphisme de F induit par u^k, donc
si P(X) \in K{[}X{]}, P(v) l'endomorphisme de F induit par P(u). En
particulier \muu(v) est l'endomorphisme de F induit par
\muu(u) = 0 donc \muu(v) = 0 et \muv divise
\muu.

Théorème~3.2.4 Les racines de \muu sont exactement les valeurs
propres de u.

Démonstration Soit \lambda~ une valeur propre de u et x un vecteur propre
associé. On a \forall~k \in \mathbb{N}~, u^k~(x) =
\lambda~^kx et donc \forall~~P \in K{[}X{]}, P(u)(x)
= P(\lambda~)x. En particulier 0 = \muu(u)(x) = \muu(\lambda~)x et
comme x\neq~0, on a \muu(\lambda~) = 0.
Inversement soit \lambda~ une racine de \muu et écrivons
\muu(X) = (X - \lambda~)Q(X). Supposons que \lambda~ n'est pas valeur propre
de u. On a 0 = \muu(u) = (u -
\lambda~\mathrmIdE) \cdot Q(u). Mais comme \lambda~ n'est pas
valeur propre de u, u - \lambda~\mathrmIdE est
in\\\\jmathmathmathmathectif et donc Q(u) = 0. Ceci impose que \muu divise Q ce qui
est impossible pour une question de degrés.

\paragraph{3.2.3 Théorème de Cayley-Hamilton}

Théorème~3.2.5 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors \chiu(u) = 0.

Remarque~3.2.1 Une version équivalente est~: soit M \in MK(n).
Alors \chiM(M) = 0.

Démonstration Démonstration 1. Soit x \in E,
x\neq~0 et soit d =\
max\k∣(x,u(x),\\ldots,u^k-1~(x))\text
libre \. On a alors u^d(x) = \lambda~0x
+ \\ldots~ +
\lambda~d-1u^d-1(x). Soit Ex
=\
\mathrmVect(x,u(x),\\ldots,u^d-1~(x)).
Alors Ex est stable par u. Soit v la restriction de u à
Ex. \mathcal{E}x =
(x,u(x),\\ldots,u^d-1~(x))
est une base de Ex et

\mathrmMat (v,\mathcal{E}x~)
= \left
(\matrix\,0&0&\\ldots&\\\ldots&\lambda~0~
\cr
1&0&\\ldots&\\\ldots&\lambda~1~
\cr
\\ldots&⋱&\mathrel⋱&\\\ldots&\\\ldots~
\cr
\\ldots&\\\ldots&⋱&\mathrel⋱&\\\ldots~
\cr
0&\\ldots&\\\ldots&1&\lambda~d-1~\right
)

Un calcul simple montre que \chiv(X) = X^d -
\lambda~d-1X^d-1
-\\ldots~ -
\lambda~0. On a donc \chiv(v)(x) = 0 et donc
\chiv(u)(x) = 0. Mais \chiv divise \chiu et donc on
a aussi \chiu(u)(x) = 0. Comme x est quelconque, la relation
\chiu(u)(x) = 0 étant évidente si x = 0, on a \chiu(u) =
0.

Démonstration 2. Montrons que si K est un sous-corps de \mathbb{C} et M \in
MK(n), alors \chiM(M) = 0. Il suffit bien entendu de le
montrer lorsque M \in M\mathbb{C}(n). Si M est diagonalisable, on a M =
P^-1DP avec D =\
\mathrmdiag(\lambda~1,\\ldots,\lambda~n~).
On a alors \chiM(M) = \chiD(M) =
P^-1\chiD(D)P =
P^-1\
\mathrmdiag(\chiD(\lambda~1),\\ldots,\chiD(\lambda~n~))P
= P^-10P = 0. Si M n'est pas diagonalisable, soit
Mn une suite de matrices diagonalisables qui converge vers M.
Comme l'application A\mapsto~\chiA(A) est
polynomiale en les coefficients de A, elle est continue et on a

\chiM(M) =\
lim\chiMn(Mn) =\
lim0 = 0

Corollaire~3.2.6 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors \muu divise \chiu.

\paragraph{3.2.4 Polynôme annulateur et trigonalisation}

Théorème~3.2.7 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors u est trigonalisable si et seulement si il existe un
polynôme P \in K{[}X{]} \diagdown 0, scindé sur K tel que P(u) = 0.

Démonstration Si u est trigonalisable, le polynôme caractéristique
\chiu de u est scindé et vérifie \chiu(u) = 0.

Inversement, supposons qu'il existe un polynôme non nul, scindé P tel
que P(u) = 0. On peut supposer que P est normalisé si bien que l'on peut
écrire P = \∏ ~
i=1^k(X - \lambda~i). On a 0
= \∏ ~
i=1^k(u - \lambda~i\mathrmId), si
bien que l'un au moins des u - \lambda~i\mathrmId
est non in\\\\jmathmathmathmathectif. Donc u possède au moins une valeur propre.

Montrons donc le résultat par récurrence sur n =\
dim E~; il n'y a rien à démontrer si n = 1. Soit \lambda~ une valeur propre
de u. Soit e1 un vecteur propre associé à \lambda~, que l'on complète
en
(e1,\\ldots,en~)
base de E. Soit F =\
\mathrmVect(e2,\\ldots,en~),
p la pro\\\\jmathmathmathmathection sur F parallèlement à Ke1 et v : F \rightarrow~ F défini
par v(x) = p(u(x)) si x \in F. Alors M =\
\mathrmMat (u,\mathcal{E}) = \left
(\matrix\,\lambda~&∗∗∗ \cr
\matrix\,0 \cr
\⋮~ \cr
0&A \right ) avec A =\
\mathrmMat
(v,(e2,\\ldots,en~)).
Le produit de matrices par blocs et une récurrence élémentaire montrent
que \forall~~p \in \mathbb{N}~,

M^p = \left
(\matrix\,\lambda~&∗∗∗ \cr
\matrix\,0 \cr
\⋮~ \cr
0&A^p \right )

et par combinaisons linéaires que

P(M) = \left
(\matrix\,\lambda~&∗ ∗ ∗ \cr
\matrix\,0 \cr
\⋮~ \cr
0&P(A)\right )

On en déduit que P(A) = 0 et comme P(A) =\
\mathrmMat
(P(v),(e2,\\ldots,en~)),
on a aussi P(v) = 0. Par hypothèse de récurrence, il existe une base
(\epsilon2,\\ldots,\epsilonn~)
de F telle que \mathrmMat~
(v,(\epsilon2,\\ldots,\epsilonn~))
soit triangulaire supérieure et alors
\mathrmMat~
(u,(e1,\epsilon2,\\ldots,\epsilonn~))
= \left (\matrix\,\lambda~&∗
\cr \matrix\,0
\cr \⋮~
\cr
0&\mathrmMat~
(v,(\epsilon2,\\ldots,\epsilonn))~\right
) est triangulaire supérieure.

\paragraph{3.2.5 Décomposition des noyaux}

Théorème~3.2.8 Soit E un K-espace vectoriel et u \in L(E). Soit P \in
K{[}X{]} et P =
P1\\ldotsPk~
une décomposition de P en produit de polynômes deux à deux premiers
entre eux. Alors
\mathrmKer~P(u)
= \mathrmKerP1~(u)
\oplus~⋯
\oplus~\mathrmKerPk~(u).

Démonstration Par récurrence sur k ≥ 2. Pour k = 2, écrivons P =
P1P2 avec P1 et P2 premiers
entre eux. Soit U et V tels que UP1 + V P2 = 1
(Bezout). On a dé\\\\jmathmathmathmathà
\mathrmKerPi~(u)
\subset~\mathrmKer~P(u). De plus on
a

U(u)P1(u) + V (u)P2(u) =
\mathrmIdE

Soit x
\in\mathrmKerP1~(u)
\bigcap\mathrmKerP2~(u).
On a x = U(u)P1(u)(x) + V (u)P2(u)(x) = 0 + 0 = 0,
donc
\mathrmKerP1~(u)
\bigcap\mathrmKerP2~(u)
= \0\. Soit maintenant x
\in\mathrmKer~P(u). On a
encore x = x1 + x2 avec x1 = V
(u)P2(u)(x) et x2 = U(u)P1(u)(x). Mais
alors

\begin{align*} P1(u)(x1)& =&
P1(u)V (u)P2(u)(x) = V
(u)(P1P2)(u)(x)\%& \\
& =& V (u)P(u)(x) = 0 \%& \\
\end{align*}

donc x1
\in\mathrmKerP1~(u).
De même x2
\in\mathrmKerP2~(u)
et donc \mathrmKer~P(u)
= \mathrmKerP1~(u)
\oplus~\mathrmKerP2~(u).
Supposons donc le résultat vrai pour k - 1. Comme Pk et
P1\\ldotsPk-1~
sont premiers entre eux, le cas k = 2 nous donne
\mathrmKer~P(u)
=\
\mathrmKerP1\\ldotsPk-1~(u)
\oplus~\mathrmKerPk~(u)
puis grâce à l'hypothèse de récurrence
\mathrmKer~P(u)
= \mathrmKerP1~(u)
\oplus~⋯
\oplus~\mathrmKerPk~(u).

Corollaire~3.2.9 Soit E un K-espace vectoriel et u \in L(E). Soit P \in
K{[}X{]} et P =
P1\\ldotsPk~
une décomposition de P en produit de polynômes deux à deux premiers
entre eux. On suppose que P(u) = 0. Alors E =\
\mathrmKerP1(u)
\oplus~⋯
\oplus~\mathrmKerPk~(u).

Proposition~3.2.10

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) Soit E un K-espace vectoriel et u \in L(E) tel que u^2 =
  u. Alors u est un pro\\\\jmathmathmathmathecteur
\item
  (ii) Soit E un K-espace vectoriel et u \in L(E) tel que u^2 =
  \mathrmId. Si
  carK\mathrel\neq~~2, alors u
  est la symétrie par rapport à un sous-espace V parallèlement à un
  supplémentaire.
\end{itemize}

Démonstration

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) On écrit X^2 - X = X(X - 1)~: les polynômes X et X - 1
  sont premiers entre eux d'où E =\
  \mathrmKer(u -\mathrmId)
  \oplus~\mathrmKer~u. Alors u est
  la pro\\\\jmathmathmathmathection sur
  \mathrmKer~(u
  -\mathrmId) parallèlement à
  \mathrmKer~u.
\item
  (ii) On écrit X^2 - 1 = (X - 1)(X + 1)~: les polynômes X +
  1 et X - 1 sont premiers entre eux (si
  carK\mathrel\neq~~2) d'où E
  = \mathrmKer~(u
  -\mathrmId)
  \oplus~\mathrmKer~(u +
  \mathrmId). Alors u est la symétrie par rapport à
  \mathrmKer~(u
  -\mathrmId) parallèlement à
  \mathrmKer~(u +
  \mathrmId).
\end{itemize}

Théorème~3.2.11 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors u est diagonalisable si et seulement si il existe P \in
K{[}X{]} scindé à racines simples tel que P(u) = 0.

Démonstration

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  ( \rigtharrow~). Soit \mathcal{E} une base de E telle que D =\
  \mathrmMat (u,\mathcal{E}) soit diagonale, D
  =\
  \mathrmdiag(\lambda~1,\\ldots,\lambda~n~).
  Supposons que
  \lambda~1,\\ldots,\lambda~k~
  sont distinctes et que pour i ≥ k + 1, \lambda~i
  \in\\lambda~1,\\ldots\lambda~k\~.
  Soit P = \∏ ~
  i=1^k(X - \lambda~i). On a P(D)
  =\
  \mathrmdiag(P(\lambda~1),\\ldots,P(\lambda~n~))
  = 0 et donc P(u) = 0 avec P scindé à racines simples.
\item
  ( ⇐) Soit P(X) =\ \∏
   i=1^k(X - \lambda~i). On a donc E
  = \mathrmKer~(u -
  \lambda~1\mathrmId)
  \oplus~⋯
  \oplus~\mathrmKer~(u -
  \lambda~k\mathrmId). En réunissant des bases de
  tous ces sous-espaces, on obtient une base de E formée de vecteurs
  propres de u. Donc u est diagonalisable.
\end{itemize}

Remarque~3.2.2 Ce théorème peut encore s'exprimer sous la forme~: u est
diagonalisable si et seulement si \muu est scindé à racines
simples.

\paragraph{3.2.6 Sous-espaces caractéristiques}

Remarque~3.2.3 Soit E un K-espace vectoriel de dimension finie, u \in L(E)
et P tel que P(u) = 0. Soit P =
P1\\ldotsPk~
une décomposition de \chiu en produit de polynômes deux à deux
premiers entre eux. Soit Ei =\
\mathrmKerPi(u). On a E = E1
\oplus~⋯ \oplus~ Ek et chacun des sous-espaces
Ei est stable par u. Soit ui la restriction de u à
Ei. Dans une base \mathcal{E} = \mathcal{E}1
\cup\\ldots~
\cup\mathcal{E}k adaptée à la décomposition en somme directe, on a M
= \mathrmMat~ (u,\mathcal{E})
=\
\mathrmdiag(M1,\\ldots,Mk~)
avec Mi =\
\mathrmMat (ui,\mathcal{E}i). On en
déduit (calcul par blocs d'un déterminant) que \chiu
= \∏ ~
\chiui. De même, on a, si Q \in K{[}X{]}, Q(M)
=\
\mathrmdiag(Q(M1),\\ldots,Q(Mk~))
et donc Q(u) = 0 \Leftrightarrow
\forall~i, Q(ui~) = 0. On en déduit que
\muu = ppcm\muui~.
Mais Pi(ui) = 0 et donc \muui
divise Pi. On en déduit que les \muui sont
deux à deux premiers entre eux et donc \muu
= \∏ ~
\muui.

Supposons maintenant que le polynôme caractéristique de u est scindé,
\chiu(X) = \\∏
 i=1^k(X - \lambda~i)^mi avec
\lambda~1,\\ldots,\lambda~k~
distincts. Appliquons les résultats précédents avec P = \chiu et
Pi = (X - \lambda~i)^mi. Ceci nous
conduit à la définition~:

Définition~3.2.2 Soit u \in L(E) et \lambda~ une valeur propre de u de
multiplicité m. Le sous-espace
\mathrmKer~(u -
\lambda~\mathrmId)^m est appelé sous-espace
caractéristique de u associé à \lambda~, il est stable par u.

Remarque~3.2.4 Appelons donc Ei le sous-espace caractéristique
de u associé à \lambda~i, et ui la restriction de u à
Ei. On sait que \muui divise (X -
\lambda~i)^mi, donc \muui = (X
- \lambda~i)^ri. Dans ce cas, \muu(X)
= \∏ ~
i=1^k(X - \lambda~i)^ri. De plus,
\chiu(X) = \∏ ~
\chiui, chacun des \chiui est scindé
et a les mêmes racines que \muui. On en déduit que
\chiui(X) = (X -
\lambda~i)^dim Ei~. Mais
alors \chiu(X) =\
∏  i=1^k~(X -
\lambda~i)^mi =\
∏ i=1^k~(X -
\lambda~i)^dim Ei~. Ceci
démontre que dim Ei = mi~.
D'où le théorème

Théorème~3.2.12 Soit E un K-espace vectoriel de dimension finie et u \in
L(E) dont le polynôme caractéristique est scindé sur K. Alors E est
somme directe des sous-espaces caractéristiques de u~; chaque
sous-espace caractéristique a pour dimension la multiplicité de la
valeur propre correspondante.

\paragraph{3.2.7 Application~: récurrences linéaires d'ordre 2}

Théorème~3.2.13 Soit E un K-espace vectoriel de dimension 2 et u \in L(E)
dont le polynôme caractéristique est scindé. Alors, il existe une base \mathcal{E}
de E telle que la matrice de u dans cette base soit de l'une des deux
formes suivantes~: \left
(\matrix\,\lambda~1&0
\cr 0 &\lambda~2\right ) ou
\left
(\matrix\,\lambda~&1\cr 0
&\lambda~\right ).

Démonstration Si u est diagonalisable, il existe une base \mathcal{E} de E telle
que la matrice de u dans cette base soit \left
(\matrix\,\lambda~1&0
\cr 0 &\lambda~2\right ). Supposons
donc u non diagonalisable. Le polynôme caractéristique de u a
nécessairement une racine double \lambda~ (sinon u serait diagonalisable) et le
sous-espace propre associé Eu(\lambda~) est nécessairement de
dimension 1 (pour la même raison). Soit donc e2 \in E \diagdown
Eu(\lambda~) et posons e1 = u(e2) - \lambda~e2
= (u - \lambda~\mathrmId)(e2)~; le vecteur
e1 est non nul car e2 n'est pas vecteur propre de u.
Le théorème de Cayley Hamilton garantit que (u -
\lambda~\mathrmId)^2 = 0 et donc (u -
\lambda~\mathrmId)(e1) = 0. Donc e1 est
vecteur propre de u. Ceci garantit que (e1,e2) est
libre (puisque e2 n'est pas vecteur propre de u), et donc est
une base de E. On a u(e1) = \lambda~e1 et u(e2) =
e1 + \lambda~e2 et donc la matrice de u dans cette base est
\left
(\matrix\,\lambda~&1\cr 0
&\lambda~\right ).

Proposition~3.2.14 Soit n \in \mathbb{N}~, alors

 \left
(\matrix\,\lambda~1&0
\cr 0 &\lambda~2\right )^n
= \left
(\matrix\,\lambda~1^n&0
\cr 0 &\lambda~2^n\right )

et

 \left
(\matrix\,\lambda~&1\cr 0
&\lambda~\right )^n = \left
(\matrix\,\lambda~^n&n\lambda~^n-1
\cr 0 &\lambda~^n \right )

Démonstration La première formule est évidente~; la deuxième peut se
montrer soit par récurrence, soit en appliquant la formule du binôme~;
on écrit que

\left
(\matrix\,\lambda~&1\cr 0
&\lambda~\right ) = \lambda~I2 + \left
(\matrix\,0&1 \cr
0&0\right )

et on remarque que \left
(\matrix\,0&1 \cr
0&0\right )^2 = 0.

Considérons a,b \in \mathbb{C}, b\neq~0 et soit E l'ensemble
des suites (un)n\in\mathbb{N}~ de nombres complexes vérifiant

\forall~n \in \mathbb{N}~, un+2 = aun+1~ +
bun

Proposition~3.2.15 E est un \mathbb{C}-espace vectoriel et l'application E \rightarrow~
\mathbb{C}^2,
(un)n\in\mathbb{N}~\mapsto~(u0,u1)
est un isomorphisme d'espaces vectoriels~; en particulier,
dim~ E = 2.

Démonstration La vérification du premier point est élémentaire.
L'application
(un)n\in\mathbb{N}~\mapsto~(u0,u1)
est visiblement linéaire et elle est bi\\\\jmathmathmathmathective car une suite de E est
entièrement déterminée par la donnée de ses deux premiers éléments.

Soit (un)n\in \mathbb{N}~ \in E et posons Un =
\left
(\matrix\,un
\cr un+1\right ). On a alors

\begin{align*} Un+1& =&
\left
(\matrix\,un+1
\cr un+2\right ) =
\left
(\matrix\,un+1
\cr aun+1 +
bun\right )\%&
\\ & =& \left
(\matrix\,0&1 \cr
b&a\right )\left
(\matrix\,un
\cr un+1\right ) =
AUn \%& \\
\end{align*}

avec A = \left
(\matrix\,0&1 \cr
b&a\right ). On en déduit que Un =
A^nU0. Comme \mathbb{C} est algébriquement clos,
\chiA est scindé. Si A est diagonalisable, il existe P inversible
telle que A = P^-1\left
(\matrix\,\lambda~1&0
\cr 0 &\lambda~2\right )P, d'où

Un = P^-1\left
(\matrix\,\lambda~1^n&0
\cr 0 &\lambda~2^n\right
)PU0

et en prenant la première coordonnée, un =
\alpha~\lambda~1^n + \beta~\lambda~2^n. Si par contre, A n'est
pas diagonalisable, il existe P inversible telle que A =
P^-1\left
(\matrix\,\lambda~&1\cr 0
&\lambda~\right )P, d'où

Un = P^-1\left
(\matrix\,\lambda~^n&n\lambda~^n-1
\cr 0 &\lambda~^n \right
)PU0

et en prenant la première coordonnée, un = \alpha~\lambda~^n +
\beta~n\lambda~^n.

On a donc E \subset~ F avec F =\
\mathrmVect((\lambda~1^n)n\in\mathbb{N}~,(\lambda~2^n)n\in\mathbb{N}~)
ou F =\
\mathrmVect((\lambda~^n)n\in\mathbb{N}~,(n\lambda~^n)n\in\mathbb{N}~)
suivant le cas. Comme dim~ E = 2 et
dim~ F \leq 2, on a nécessairement égalité.
Remarquons alors que, pour \lambda~\neq~0,

(\lambda~^n) n\in\mathbb{N}~ \in E\quad
\Leftrightarrow \lambda~^2 = a\lambda~ + b
\Leftrightarrow \chi A(\lambda~) = 0

Ceci nous conduit à la méthode suivante de résolution de la récurrence
linéaire \forall~n \in \mathbb{N}~, un+2~ =
aun+1 + bun

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  rechercher les solutions particulières de la forme un =
  \lambda~^n ceci conduit à une équation du second degré en \lambda~, P(\lambda~)
  = 0
\item
  si cette équation a deux racines simples \lambda~1 et \lambda~2,
  les solutions sont les suites de la forme un =
  \alpha~\lambda~1^n + \beta~\lambda~2^n
\item
  si cette équation a une racine double \lambda~, les solutions sont les suites
  de la forme un = \alpha~\lambda~^n + \beta~n\lambda~^n.
\end{itemize}

{[}
{[}
{[}
{[}
