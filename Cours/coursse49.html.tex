\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

{[}
{[}
{[}{]}
{[}

\subsubsection{8.6 Analyse numérique des fonctions d'une variable}

\paragraph{8.6.1 Interpolation linéaire, interpolation polynomiale}

Considérons f une fonction de classe C^n sur l'intervalle
{[}a,b{]}, soit x1 \textless{}
\\ldots~ \textless{}
xn des points de {[}a,b{]} et considérons l'unique polynôme P
\in \mathbb{R}~n-1{[}X{]} vérifiant P(xi) = f(xi) pour
1 \leq i \leq n (polynôme d'interpolation de Lagrange).

Lemme~8.6.1 Pour tout x \in {[}a,b{]}, \exists~\zeta
\in{]}a,b{[}, f(x) - P(x) =
(x-x1)\\ldots(x-xn~)
\over n! f^(n)(\zeta).

Démonstration Si x est l'un des xi, n'importe quel \zeta convient.
On peut donc supposer que f n'est pas l'un des xi. Considérons
alors g : t\mapsto~f(t) - P(t) - \lambda~
(t-x1)\\ldots(t-xn~)
\over n! où \lambda~ est choisi de telle sorte que g(x) = 0
(c'est évidemment possible). Alors g est de classe C^n et a n
+ 1 zéros distincts sur l'intervalle {[}a,b{]}. Des applications
répétées du théorème de Rolle assurent que la fonction g' a n zéros
distincts sur l'intervalle {]}a,b{[} (un entre deux zéros de g au sens
strict), puis que g'' a n - 1 zéros distincts \\\\jmathmathmathmathusque g^(n)
qui a au moins un zéro. Soit donc \zeta tel que g^(n)(\zeta) = 0. On
a donc 0 = g^(n)(\zeta) = f^(n)(\zeta) - \lambda~ car
P^(n) = 0 (vu que deg~ P \leq n - 1) et
 d^n \over dt^n ((t -
x1)\\ldots~(t
- xn)) = n!. on a donc \lambda~ = f^(n)(\zeta) et en écrivant
que g(x) = 0, on obtient f(x) - P(x) =
(x-x1)\\ldots(x-xn~)
\over n! f^(n)(\zeta).

Considérons le cas particulier où n = 2 et où l'on prend x1 =
a et x2 = b~; c'est le cas de l'interpolation linéaire entre a
et b où l'on remplace la courbe y = f(x) par la corde \\\\jmathmathmathmathoignant les
points (a,f(a)) et (b,f(b)). On a alors P(t) = f(a) + f(b)-f(a)
\over b-a (t - a). Si on appelle M2
=\
supt\in{[}a,b{]}\textbar{}f''(t)\textbar{}, on obtient

Proposition~8.6.2 (erreur dans une interpolation linéaire). Soit f :
{[}a,b{]} \rightarrow~ \mathbb{R}~ de classe C^2, M2
=\
supt\in{[}a,b{]}\textbar{}f''(t)\textbar{}. Alors

\forall~~t \in {[}a,b{]}, \textbar{}f(t)
-\left (f(a) + f(b) - f(a) \over b - a
(t - a)\right )\textbar{}\leq M2 (b -
a)^2 \over 8

Démonstration On a en effet

\begin{align*} \textbar{}f(x) - P(x)\textbar{}&
=& (x - a)(b - x) \over 2
\textbar{}f''(\zeta)\textbar{}\leq M2 (x - a)(b - x)
\over 2 \%& \\ & \leq&
M2 (b - a)^2 \over 8 \%&
\\ \end{align*}

car (x - a)(b - x) \leq (b-a)^2 \over 4 pour
x \in {[}a,b{]}.

\paragraph{8.6.2 Dérivation numérique}

Nous nous limiterons au calcul approché des dérivées d'ordre 1 et 2
d'une fonction numérique. Pour la dérivée d'ordre 1 nous utiliserons la
formule f(x + h) = f(x) + hf'(x) + h\epsilon(h) avec
limh\rightarrow~0~\epsilon(h) = 0. On en déduit que
f'(x) est peu différent de \Deltahf(x) = f(x+h)-f(x)
\over h quand h est petit. Mathématiquement, plus h est
petit, plus \Deltahf(x) est proche de f'(x). Mais qu'en est-il de
la valeur calculée \overline\Deltahf(x))~? Le
calcul de f(x + h) - f(x) se fait avec une erreur absolue de l'ordre de
2\deltaf(x), où \delta est la précision de l'instrument de calcul (
10^-16 par exemple). On a donc \textbar{}\Deltahf(x)
-\overline\Deltahf(x))\textbar{}\leq
2\delta\textbar{}f(x)\textbar{} \over h , qui tend vers + \infty~
quand h tend vers 0. Il faut donc trouver un compromis pour la valeur de
h à choisir. Supposons f de classe C^2. Alors on a f(x + h) =
f(x) + hf'(x) + h^2 \over 2 f''(x) +
h^2\epsilon(h) et donc \textbar{}f'(x) - \Deltahf(x)\textbar{}
est peu différent de  h \over 2
\textbar{}f''(x)\textbar{}. On a donc \textbar{}f'(x)
-\overline\Deltahf(x))\textbar{}\leq h
\over 2 \textbar{}f''(x)\textbar{} +
2\delta\textbar{}f(x)\textbar{} \over h . Le deuxième membre
est une fonction de h qui est minimale pour h =
2\sqrt \delta\textbar{}f(x)\textbar{} \over
\textbar{}f''(x)\textbar{}  et qui vaut alors
2\sqrt\delta\textbar{}f(x)f''(x)\textbar{}. Avec les
fonctions usuelles, on retiendra qu'il faut choisir un h de l'ordre de
\sqrt\delta (plutôt un peu trop grand, qu'un peu trop
petit) et que l'on obtient alors une erreur de l'ordre de
\sqrt \delta. Ainsi on choisira par exemple h =
10^-8 et on pourra espérer avoir 7 ou 8 chiffres
significatifs dans le calcul de la dérivée (ce qui est le plus souvent
largement suffisant, par exemple pour une étude de fonction).

Une autre méthode de calcul de la dérivée qui donne des valeurs un peu
plus précises (mais peut poser des problèmes de définition de la
fonction aux bornes de l'intervalle) est de prendre comme valeur
approchée de la dérivée l'expression  f(x+h)-f(x-h)
\over 2h (dérivée symétrique). L'erreur est alors en 
h^2 \over 6
\textbar{}f^(3)(x)\textbar{} + 2\delta\textbar{}f(x)\textbar{}
\over h , elle est minimale pour un h de l'ordre de
\root3\of\delta et elle est alors de
l'ordre de \delta^2\diagup3 (prendre h = 10^-5 pour obtenir
environ 9 à 10 chiffres significatifs).

Pour le calcul de la dérivée seconde, reprenons la formule de
Taylor-Young f(x + h) = f(x) + hf'(x) + h^2
\over 2 f''(x) + h^2\epsilon(h). On obtient alors
limh\rightarrow~0~ f(x+h)+f(x-h)-2f(x)
\over h^2 = f''(x). On utilisera comme
valeur approchée de f''(x) l'expression \Deltah^(2)f(x) =
f(x+h)+f(x-h)-2f(x) \over h^2 . Utilisons la
même méthode pour évaluer l'erreur entre la valeur calculée de
\Deltah^(2)f(x) et f''(x). Supposons f de classe
C^4. On a la formule de Taylor Young à l'ordre 4, f(x + h) =
f(x) + hf'(x) + h^2 \over 2 f''(x) +
h^3 \over 6 f^(3)(x) +
h^4 \over 24 f^(4)(x) +
h^4\epsilon(h) d'où \textbar{}\Deltah^(2)f(x) -
f''(x)\textbar{} est peu différent de  h^2
\over 12 \textbar{}f^(4)(x)\textbar{}. On
obtient donc une ma\\\\jmathmathmathmathoration du type
\textbar{}\overline\Deltah^(2)f(x) -
f''(x)\textbar{}\leq h^2 \over 12
\textbar{}f^(4)(x)\textbar{} + 3\delta \over
h^2 \textbar{}f(x)\textbar{}. L'erreur est minimale pour
une valeur de h de l'ordre de
\root4\of\delta. On choisira donc un h
de l'ordre de 10^-4 pour obtenir un résultat optimal pour les
fonctions usuelles.

\paragraph{8.6.3 Recherche des zéros d'une fonction}

On suppose dans toutes les méthodes qui suivent que l'on a effectué la
séparation des zéros de la fonction f, c'est-à-dire que l'on a trouvé un
intervalle {[}a,b{]} sur lequel f est strictement monotone avec f(a)f(b)
\textless{} 0. On suppose également que f est suffisamment dérivable. On
sait alors que f a un unique zéro r sur l'intervalle {[}a,b{]}.

Méthode de dichotomie

Soit c un point de {]}a,b{[}. Alors, soit f(a) et f(c) sont de signe
contraire, auquel cas r \in{]}a,c{[}, soit f(a) et f(c) sont de même signe
et dans ce cas r \in{]}c,a{[}. En prenant c = a+b \over
2 et en itérant le procédé, on construit une suite de segments
emboîtés {[}an,bn{]} tels que
\forall~n \in \mathbb{N}~, r \in {[}an,bn~{]} et
bn - an = 1 \over 2
(bn-1 - an-1), soit bn - an =
1 \over 2^n (b - a). Le théorème des
segments emboîtés garantit alors que
\⋂ ~
n\in\mathbb{N}~{[}an,bn{]} = r. Si l'on prend
an comme valeur approchée de r par exemple, on a \textbar{}r -
an\textbar{}\leq 1 \over 2^n (b -
a).

Méthode de Lagrange

La méthode de Lagrange consiste à assimiler sur le segment {[}a,b{]} la
courbe y = f(x) à la droite passant par les points (a,f(a)) et (b,f(b)),
c'est-à-dire à approcher f par la fonction P(x) = f(a) + f(b)-f(a)
\over b-a (x - a) et à prendre comme valeur approchée
de r le réel \barr vérifiant
P(\barr) = 0.

Etudions l'erreur commise dans cette méthode. Soit P(x) = f(a) +
f(b)-f(a) \over b-a (x - a) (interpolation linéaire).
La ma\\\\jmathmathmathmathoration de l'erreur dans une interpolation linéaire nous garantit
que, en posant M2 =\
supt\in{[}a,b{]}\textbar{}f''(t)\textbar{}, on a \textbar{}f(r)
- P(r)\textbar{}\leq M2 (b-a)^2 \over
8 . Mais f(r) = 0 et  P(r) \over
r-\barr = P(r)-P(\barr)
\over r-\barr = f(b)-f(a)
\over b-a = f'(c) pour un c \in{]}a,b{[}. On en déduit
que \textbar{}\barr - r\textbar{} =
\left \textbar{} P(r) \over f'(c)
\right \textbar{}\leq M2(b-a)^2
\over 8m1 si l'on pose m1
= inf~
t\in{[}a,b{]}\textbar{}f'(t)\textbar{} (que nous supposerons
strictement positif). D'où la ma\\\\jmathmathmathmathoration de l'erreur dans la méthode de
Lagrange

\textbar{}\barr - r\textbar{}\leq M2(b -
a)^2 \over 8m1

On pourra par exemple combiner la méthode de dichotomie et la méthode de
Lagrange pour trouver rapidement une bonne approximation de r.
Remarquons de plus que si on suppose en outre que f est de concavité
constante sur {[}a,b{]}, alors on connait le signe de r
-\bar r. En effet

(f convexe croissante, ou f concave décroissante))
\rigtharrow~\bar r \textless{} r

(f convexe décroissante, ou f concave croissante))
\rigtharrow~\bar r \textgreater{} r

Méthode de Newton

La méthode de Newton consiste à assimiler la courbe y = f(x) à la
tangente en un point c \in {[}a,b{]}. Cette tangente a pour équation y -
f(c) = f'(c)(x - c). Elle coupe donc l'axe des x au point d'abscisse r'
= c - f(c) \over f'(c) .

Cherchons à ma\\\\jmathmathmathmathorer l'erreur commise \textbar{}r - r'\textbar{}. On a
d'après la formule de Taylor Lagrange 0 = f(r) = f(c) + (r - c)f'(c) +
(r-c)^2 \over 2 f''(x) pour un certain x
\in{]}r,c{[}. De plus on a f(c) + (r' - c)f'(c) = 0. En soustrayant membre
à membre les deux égalités on trouve (r' - r)f'(c) -
(r-c)^2 \over 2 f''(x) = 0, soit r' - r =
(r-c)^2 \over 2  f'`(x)
\over f'(c) et donc (avec les notations données ci
dessus dans la méthode de Lagrange)

\textbar{}r' - r\textbar{}\leq M2(b - a)^2
\over 2m1

Remarquons que la ma\\\\jmathmathmathmathoration de l'erreur obtenue est 4 fois plus grande
que dans la méthode de Lagrange. Ce n'est évidemment pas que la méthode
de Newton soit moins bonne que la méthode de Lagrange, c'est simplement
la ma\\\\jmathmathmathmathoration de l'erreur qui est un peu moins fine. Remarquons que si
l'on suppose en outre que f est de concavité constante sur {[}a,b{]},
alors on connait le signe de r' - r. En effet

(f convexe croissante, ou f concave décroissante)) \rigtharrow~ r' \textgreater{} r

(f convexe décroissante, ou f concave croissante)) \rigtharrow~ r' \textless{} r

Les inégalités sont donc en sens contraire de celles obtenues par la
méthode de Lagrange. Dans le cas où f est monotone et de concavité
constante sur {[}a,b{]}, la combinaison de la méthode de Lagrange et de
la méthode de Newton fournit un encadrement de r, ce qui est meilleur
qu'une ma\\\\jmathmathmathmathoration d'erreur.

Dans la pratique on ne s'arrête pas après avoir appliqué une fois la
méthode de Newton, mais au contraire on applique de nouveau la méthode
de Newton, mais cette fois ci au point r'. Cela revient à construire une
suite (xn) par récurrence par xo = c et
xn+1 = xn - f(xn) \over
f'(xn) . Les questions qui se posent naturellement sont

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) est ce que tous les xn sont dans {[}a,b{]}~? ,
\item
  (ii) est ce que la suite (xn) converge~?
\item
  (iii) dans ce cas, sa limite est-elle r~?
\end{itemize}

Il est clair que dans la mesure où les réponses aux questions (i) et
(ii) sont oui, la réponse à la question (iii) est aussi oui, puisque si
l'on appelle L la limite de la suite, on doit avoir L = L - f(L)
\over f'(L) , soit f(L) = 0. Il nous reste donc à
répondre aux deux premières questions.

Nous nous placerons sous les hypothèses suivantes~: f est de classe
C^2 sur {[}a,b{]}, f' ne s'annule pas sur {[}a,b{]} et f''
est de signe constant sur {[}a,b{]} (donc f est strictement monotone et
de concavité constante). Dans un premier temps nous supposerons pour
faire les raisonnements que f est croissante convexe sur {[}a,b{]}.
Prenons xo = c \textgreater{} r (par exemple c = b). On va
alors montrer que \forall~n, xn~ \in {[}r,b{]}
et que la suite (xn) est décroissante, ce qui permettra de
répondre positivement aux questions (i) et (ii). Pour cela considérons
la fonction g définie par g(x) = x - f(x) \over f'(x)
= xf'(x)-f(x) \over f'(x) . On a g'(x) = f(x)f'`(x)
\over f'(x)^2 ≥ 0 donc g est croissante sur
{[}a,b{]}. Supposons que xn \in {[}r,b{]}. Comme g(r) = r, on a
xn+1 = g(xn) ≥ g(r) = r. De plus f étant strictement
croissante, elle est positive sur {[}r,b{]} et donc xn+1 =
xn - f(xn) \over f'(xn)
\leq xn, d'où r \leq xn+1 \leq xn \leq b. On en déduit
que les réponses aux questions (i) et (ii) sont positives et fournissent
donc un moyen d'approximation de r. Remarquons qu'il est fondamental
pour cela d'avoir choisi un c \textgreater{} r, car g est décroissante
sur {[}a,r{]} et donc si xo = c \textless{} r, x1 =
g(xo) ≥ g(r) = r, mais plus rien ne garantit que x1
appartient tou\\\\jmathmathmathmathours à {[}a,b{]}. Dans les cas de monotonie ou de
concavité différents on a les conclusions suivantes

(f convexe croissante ou f concave décroissante)~: choisir xo
\textgreater{} r~; la suite (xn) est décroissante et converge
vers r

(f convexe décroissante ou f concave croissante)~: choisir xo
\textless{} r~; la suite (xn) est croissante et converge vers
r

Il nous reste à savoir avec quelle vitesse la suite converge. On a g'(r)
= 0. Puisque g est continue, si l'on se donne K \textless{} 1, il existe
h \textgreater{} 0 tel que \forall~~x \in {[}r,r + h{]},
\textbar{}g'(x)\textbar{} \textless{} K. Alors soit N tel que n
\textgreater{} N \rigtharrow~ xn \in {[}r,r + h{]}. On a alors pour n
\textgreater{} N, \textbar{}xn+1 - r\textbar{} =
\textbar{}g(xn) - g(r)\textbar{}\leq K\textbar{}xn -
r\textbar{}, d'où pour n \textgreater{} N, \textbar{}xn -
r\textbar{}\leq K^n-N\textbar{}xN - r\textbar{}. On en
déduit que la suite (xn - r) est négligeable devant la suite
K^n pour tout K \textless{} 1, et on a donc une convergence
extrêmement rapide dès que l'on se rapproche de r. On peut d'ailleurs
trouver un équivalent de \textbar{}xn - r\textbar{} dans le
cas où g''(r)\neq~0 et montrer que
\textbar{}xn - r\textbar{}∼ K^2^n  avec
un K \textless{} 1. On a donc une convergence très rapide (de type
hyperexponentiel)~: en gros le nombre de décimales double à chaque
itération dans la limite de la précision de la machine.

Méthode des approximations successives

Elle consiste à transformer une équation du type f(x) = 0 en une
équation du type g(x) = x à laquelle on essayera d'appliquer une méthode
de point fixe~: on construit une suite xn+1 =
g(xn)~; si cette suite converge, elle converge vers un point
fixe de g. Le lecteur pourra remarquer que la méthode de Newton en est
un cas particulier avec g(x) = x - f(x) \over f'(x) .

{[}
{[}
{[}
{[}
