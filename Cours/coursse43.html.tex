\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

{[}
{[}
{[}{]}
{[}

\subsubsection{7.9 Compléments~: développements asymptotiques, analyse
numérique}

\paragraph{7.9.1 Calcul approché de la somme d'une série}

L'idée naturelle est d'approcher la somme S de la série convergente
\\sum  xn~ par
une somme partielle SN =\
\sum  n=0^Nxn~.
L'erreur de méthode est évidemment égale à RN
= \\sum ~
n=N+1^+\infty~xn. Bien entendu, à cette erreur de
méthode vient s'a\\\\jmathmathmathmathouter une erreur de calcul de la somme SN
que l'on peut estimer ma\\\\jmathmathmathmathorée par N\epsilon où \epsilon est la précision de
l'instrument de calcul. Entre la valeur cherchée S et la valeur calculée
\overlineSN il y a donc une erreur du type
\textbar{}S
-\overlineSN\textbar{}\leq\textbar{}RN\textbar{}
+ N\epsilon = \delta(N) que l'on cherchera donc à minimiser (la fonction \delta tend
manifestement vers + \infty~ quand N croît indéfiniment).

Etudions pour cela deux cas. Dans le premier cas, la série est à
convergence géométrique~: \textbar{}xn\textbar{}\leq
A\rho^n avec \rho \textless{} 1. Alors RN \leq
B\rho^N et \delta(N) \leq \delta1(N) = B\rho^N + N\epsilon. On a
\delta1'(t) = B(log \rho)\rho^t~ + \epsilon
qui s'annule pour t = t0 = 1 \over \rho
 log~ \left \textbar{} \epsilon
\over B log \rho~
\right \textbar{}. On a intérêt à choisir N aussi proche
que possible de t0 où la fonction \delta1 atteint son
minimum.

Exemple~7.9.1 ~: \epsilon = 10^-8,B = 1,\rho = 9 \over
10 . On trouve un N de l'ordre de 150 pour une erreur de l'ordre de
10^-5. C'est parfaitement raisonnable.

Dans le second cas, la série est à convergence polynomiale~:
\textbar{}xn\textbar{}\leq A \over
n^\alpha~ avec \alpha~ \textgreater{} 1. Alors RN \leq B
\over n^\alpha~-1 et \delta(N) \leq \delta1(N) = B
\over N^\alpha~-1 + N\epsilon. On a \delta1'(t) =
B(1 - \alpha~)t^-\alpha~ + \epsilon qui s'annule pour t = t0 =
\left ( B(\alpha~-1) \over \epsilon
\right )^ 1 \over \alpha~ . On a
intérêt à choisir N aussi proche que possible de t0 où la
fonction \delta1 atteint son minimum.

Exemple~7.9.2 ~: \epsilon = 10^-8,B = 1,\alpha~ = 11
\over 10 . On trouve un N de l'ordre de 10^7
pour une erreur de l'ordre de 0,25. On voit que la méthode fournit un
résultat très médiocre en un temps très long~; elle demande donc à être
améliorée par une accélération de convergence.

\paragraph{7.9.2 Accélération de la convergence}

Supposons que xn admet un développement asymptotique de la
forme

xn = ao \over n^K +
a1 \over n^K+1 +
\\ldots~ +
aN \over n^K+N + \epsilonn

avec \textbar{}\epsilonn\textbar{}\leq A \over
n^K+N+1 . Posons un = bo
\over n^K-1 +
\\ldots~ +
bN \over n^K+N-1 (où
bo,\\ldots,bN~
sont des coefficients à déterminer) puis yn = un -
un+1 , et cherchons à déterminer les bi de telle
sorte que \textbar{}xn - yn\textbar{}\leq B
\over n^K+N+1 (pour une certaine constante
B), c'est-à-dire, xn - yn = O( 1
\over n^K+N+1 ). On a un
= \\sum ~
i=0^N bi \over
n^K+i-1 , d'où

\begin{align*} yn& =&
\sum i=0^Nb~
i\left ( 1 \over n^K+i-1
- 1 \over (n + 1)^K+i-1
\right ) \%& \\ & =&
\sum i=0^Nb i~ 1
\over n^K+i-1 \left (1 - (1
+ 1 \over n )^1-K-i\right
)\%& \\ \end{align*}

On sait que la fonction f\alpha~(x) = (1 + x)^\alpha~ admet au
voisinage de 0 un développement limité f\alpha~(x) = 1
+ \\sum ~
k=1^pck^(\alpha~)x^k +
O(x^p+1) avec ck^(\alpha~) =
\alpha~(\alpha~-1)\\ldots~(\alpha~-k+1)
\over k! . On en déduit que

1 - (1 + 1 \over n )^1-K-i =
-\sum k=1^N+1-ic~
k^(1-K-i) 1 \over n^k + O( 1
\over n^N+2-i )

soit

\begin{align*} 1 \over
n^K+i-1 \left (1 - (1 + 1
\over n )^1-K-i\right )& =&
-\sum k=1^N+1-ic~
k^(1-K-i) 1 \over n^k+K+i-1 +
O( 1 \over n^N+K+1 )\%&
\\ & =& -\\sum
k=i^Nc k+1-i^(1-K-i) 1
\over n^k+K + O( 1 \over
n^N+K+1 ) \%& \\
\end{align*}

après changement d'indices. On en déduit

\begin{align*} yn& =&
-\sum i=0^Nb i~
\sum k=i^Nc~
k+1-i^(1-K-i) 1 \over n^k+K +
O( 1 \over n^N+K+1 )\%&
\\ & =& -\\sum
k=0^N 1 \over n^k+K 
\sum i=0^kb~
ick+1-i^(1-K-i) + O( 1 \over
n^N+K+1 )\%& \\
\end{align*}

Donc

xn - yn = O( 1 \over
n^K+N+1 ) \Leftrightarrow
\forall~k \in {[}0,n{]}, ak~ +
\sum i=0^kb~
ic1-k-i^(1-K-i) = 0

Il s'agit d'un système triangulaire en les inconnues bi qui
admet une unique solution. En faisant le changement d'indice \\\\jmathmathmathmath = k + 1 -
i, on obtient le système

\forall~k \in {[}0,n{]}, ak~ +
\sum \\\\jmathmathmathmath=1^k+1b~
k+1-\\\\jmathmathmathmathc\\\\jmathmathmathmath^(-K-k+\\\\jmathmathmathmath) = 0

On calcule donc les bk à l'aide de la formule de récurrence
c1^(-K-k+1)bk = -ak
-\\sum ~
\\\\jmathmathmathmath=2^k+1bk+1-\\\\jmathmathmathmathc\\\\jmathmathmathmath^(-K-k+\\\\jmathmathmathmath) où
les c\\\\jmathmathmathmath^(t+\\\\jmathmathmathmath) sont définis par récurrence par
c1^(t+1) = t + 1 et c\\\\jmathmathmathmath+1^(t+\\\\jmathmathmathmath+1) =
t+\\\\jmathmathmathmath+1 \over \\\\jmathmathmathmath+1 c\\\\jmathmathmathmath^(t+\\\\jmathmathmathmath). Supposons
les bi déterminés. Il existe une constante B telle que
\textbar{}xn - yn\textbar{}\leq B
\over n^K+N+1 . L'erreur faite en approchant
la somme de la série \\\sum
 (xn - yn) par sa somme partielle d'indice n est
donc ma\\\\jmathmathmathmathorée par  B \over K+N  1
\over n^K+N . Mais la somme partielle
d'indice n de la série est

\sum k=1^n(x k~ -
yk) = \\sum
k=1^nx k -\\sum
k=1^n(u k - uk+1) = Sn +
u1 - un+1

et la somme de la série est

\sum n=1^+\infty~(x n~ -
yn) = \\sum
n=1^+\infty~x n -\\sum
n=1^+\infty~(u n - un+1) = S -
u1

(puisque limun~ = 0). On a donc
\textbar{}S - Sn + un+1\textbar{}\leq B
\over K+N  1 \over n^K+N
et Sn - un+1 est donc une bien meilleure valeur
approchée de S que Sn.

Bien entendu ces méthodes peuvent se généraliser à d'autres types de
développements asymptotiques~: l'idée générale étant de trouver une
suite un telle que la série xn - (un -
un+1) ait une décroissance vers 0 aussi rapide que possible.
Alors Sn - un+1 est donc une bien meilleure valeur
approchée de S que Sn. Cette méthode fournira également des
développements asymptotiques de restes de séries car si xn -
(un - un+1) = o(vn), on aura
Rn(x) + un+1 = o(Rn(v)) et donc le
développement Rn(x) = -un+1 + o(Rn(v)).

En ce qui concerne les développements asymptotiques de sommes partielles
de séries divergentes, on se ramènera à la situation précédente en
rempla\ccant la série xn par une série du
type yn = xn - (vn - vn-1) de
telle sorte que la série
\\sum  yn~
converge. On aura alors Sn(x) = vn - v0 +
Sn(y) = vn + A + Rn(y) où A = S(y) -
v0 est une constante (sa valeur ne pourra pas être obtenue
directement par cette méthode). Il suffira ensuite d'appliquer la
méthode précédente pour obtenir un développement asymptotique de
Rn(y) à la précision souhaitée, et donc aussi un développement
asymptotique de Rn(x).

Nous allons traiter deux exemples importants des techniques ci dessus.

Exemple~7.9.3 On recherche un développement asymptotique de
\\sum ~
k=1^n 1 \over k . Posons xn
= 1 \over n et yn =\
log (n) - log~ (n - 1) =
-log (1 - 1 \over n~ ). On a
zn = xn - yn = 1 \over
n - log (1 - 1 \over n~ )
= - 1 \over 2n^2 + O( 1
\over n^3 ). On en déduit que la série
\\sum  zn~
converge. On a alors

\begin{align*} \\sum
k=1^nx k& =& 1 +
\sum k=2^nz k~ +
\sum k=2^ny k~ = 1 +
\sum k=2^nz k~ +
\sum k=2^n~(log k - log (k -
1))\%& \\ & =&
log~ n + (1 + \\sum
k=2^+\infty~z k) - Rn(z)
\%&\\ \end{align*}

Mais les théorèmes de comparaison des séries à termes de signes
constants assurent que puisque zn ∼- 1 \over
2n^2 , on a Rn(z) ∼- 1 \over 2
 \\sum ~
k=n+1^+\infty~ 1 \over k^2 ∼- 1
\over 2n . Posons alors \gamma = 1
+ \\sum ~
k=2^+\infty~zk (la constante d'Euler)~; on obtient

\sum k=1^n~ 1
\over k = log n + \gamma + 1 \over 2n +
o( 1 \over n )

(en fait il est clair que les techniques ci dessus permettent d'obtenir
un développement à un ordre arbitraire).

Exemple~7.9.4 Nous allons maintenant montrer la formule de Stirling, n!
∼\sqrt2\pi~n n^n \over
e^n . Pour cela posons an = n!e^n
\over n^n+1\diagup2 et bn
= log an~ -\
log an-1 (pour n ≥ 2). On a

\begin{align*} bn& =&
log  an~ \over
an-1 = log  n!e^n~(n
- 1)^n-1\diagup2 \over (n -
1)!e^n-1n^n+1\diagup2 \%&
\\ & =& log~
\left (e (n - 1)^n-1\diagup2 \over
n^n-1\diagup2 \right ) = 1 + (n - 1
\over 2 )log~ (1 - 1
\over n )\%& \\
\end{align*}

d'où bn = 1 + (n - 1 \over 2 )(- 1
\over n - 1 \over 2n^n
- 1 \over 3n^3 + O( 1
\over n^4 )) = - 1 \over
12n^2 + O( 1 \over n^3 ) On
en déduit que la série \\\sum
 bn converge. Soit S sa somme. On a alors
\\sum ~
k=2^nbk = S - Rn(b), mais comme
bn ∼- 1 \over 12n^2 , on a
Rn(b) ∼- 1 \over 12
 \\sum ~
k=n+1^+\infty~ 1 \over k^2 ∼- 1
\over 12n . On a d'autre part
\\sum ~
k=2^nbk = log~
an - log a1~, d'où
finalement log an~
= \\sum ~
k=2^nbk + log~
a1 = S + log a1~ + 1
\over 12n + o( 1 \over n ) et donc
an = e^S+log~
a1 exp~ ( 1 \over
12n + o( 1 \over n )) = \ell(1 + 1
\over 12n + o( 1 \over n )) en
posant \ell = e^S+log a1~
\textgreater{} 0, soit encore

n! = \ell n^n+1\diagup2 \over n!
\left (1 + 1 \over 12n + o( 1
\over n )\right )

La méthode précédente ne permet pas d'obtenir la valeur de \ell~; on
obtient celle ci classiquement à l'aide des intégrales de Wallis~:
In =\int ~
0^\pi~\diagup2 sin ^n~x dx.
Pour n ≥ 2, on écrit à l'aide d'une intégration par parties, en
intégrant sin~ x et en dérivant
sin ^n-1~x

\begin{align*} In& =&
\int ~
0^\pi~\diagup2 sin~
^n-1xsin~ x dx \%&
\\ & =& \left
{[}-cos x\sin~
^n-1x\right {]} 0^\pi~\diagup2 + (n -
1)\int ~
0^\pi~\diagup2 sin~
^n-2xcos ^2~x dx \%&
\\ & =& (n -
1)\int ~
0^\pi~\diagup2 sin ^n-2~x(1
- sin ^2x) dx = (n - 1)(I~
n-2 - In)\%& \\
\end{align*}

d'où In = n-1 \over n In-2. En
tenant compte de I0 = \pi~ \over 2 et
I1 = 1, on a alors

I2p = (2p - 1)(2p -
3)\\ldots~3.1
\over (2p)(2p -
2)\\ldots4.2~  \pi~
\over 2 = (2p)! \over
2^p(p!)^2  \pi~ \over 2

en multipliant numérateur et dénominateur par (2p)(2p -
2)\\ldots~4.2 de
manière à rétablir les facteurs manquant au numérateur. De même

I2p+1 = (2p)(2p -
2)\\ldots~4.2
\over (2p + 1)(2p -
1)\\ldots3~ =
2^p(p!)^2 \over (2p + 1)!

On en déduit en utilisant n! ∼ \ell\sqrtn
n^n \over n!

 I2p \over I2p+1 = (2p +
1)(2p)!^2 \over
2^4pp!^4  \pi~ \over 2 ∼ (2p +
1)\ell^2(2p)(2p)^4pe^4p
\over
2^4pe^4p\ell^4p^2p^4p
 \pi~ \over 2 ∼ 2\pi~ \over
\ell^2

Mais d'autre part, on a \forall~~x \in {[}0, \pi~
\over 2 {]}, 0 \leq sin~
^n+1x \leq sin ^n~x
\leq sin ^n-1~x, soit en intégrant 0 \leq
In+1 \leq In \leq In-1 et en tenant compte de 
In-1 \over In+1 = n+1
\over n , on obtient 1 \leq In
\over In+1 \leq n+1 \over n
soit encore lim In~
\over In+1 = 1. On en déduit que  2\pi~
\over \ell^2 = 1 et comme \ell \textgreater{} 0, \ell
= \sqrt2\pi~ ce qui achève la démonstration.

{[}
{[}
{[}
{[}
